{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI이미지 모델의 입력으로 이미지를 넣기 전 전처리(크기 조정, 텐서 변환) 를 하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\82103\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "mov3 = models.mobilenet_v3_small(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mov3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "\n",
    "# MobileNetV3 불러오기\n",
    "model = models.mobilenet_v3_small(pretrained=True)\n",
    "model.classifier[3] = torch.nn.Linear(in_features=1024, out_features=4)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pillow\n",
    "#pip install tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
    "import cv2 as cv\n",
    "\n",
    "inputs = cv.imread('../imgs/viewimage.png', cv.IMREAD_COLOR)\n",
    "original = inputs\n",
    "inputs = tf.expand_dims(inputs, axis=0)\n",
    "x = tf.keras.layers.experimental.preprocessing.Resizing(224, 224)(inputs)\n",
    "x = tf.keras.applications.mobilenet_v3.preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 224, 224, 3), dtype=float32, numpy=\n",
       "array([[[[ 48.086174,  49.086174,  39.688854],\n",
       "         [ 49.913425,  52.913425,  37.913425],\n",
       "         [ 47.999283,  53.972496,  31.026068],\n",
       "         ...,\n",
       "         [ 60.053432,  90.05343 ,  95.05343 ],\n",
       "         [ 54.712948,  84.71295 ,  89.71295 ],\n",
       "         [ 50.616333,  83.24034 ,  86.992325]],\n",
       "\n",
       "        [[ 45.144215,  46.144215,  36.14421 ],\n",
       "         [ 51.000397,  54.000397,  38.808434],\n",
       "         [ 47.792328,  52.77894 ,  31.805723],\n",
       "         ...,\n",
       "         [ 57.169827,  86.16982 ,  91.16982 ],\n",
       "         [ 56.031242,  85.1611  ,  90.1611  ],\n",
       "         [ 54.345432,  85.141884,  88.93644 ]],\n",
       "\n",
       "        [[ 51.26323 ,  53.26323 ,  41.26323 ],\n",
       "         [ 62.481655,  65.48166 ,  49.698093],\n",
       "         [ 94.43303 ,  99.41963 ,  78.44642 ],\n",
       "         ...,\n",
       "         [ 50.121544,  76.121544,  82.121544],\n",
       "         [ 53.703186,  78.51998 ,  84.51998 ],\n",
       "         [ 56.76253 ,  82.66514 ,  88.50369 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 65.84383 , 106.84383 , 108.95024 ],\n",
       "         [ 71.129295, 107.79678 , 111.899605],\n",
       "         [ 78.78362 , 112.25874 , 112.871895],\n",
       "         ...,\n",
       "         [106.05393 , 144.76285 , 167.2913  ],\n",
       "         [118.21729 , 151.7662  , 172.56967 ],\n",
       "         [125.233246, 155.89835 , 176.22855 ]],\n",
       "\n",
       "        [[ 83.66837 , 118.28311 , 121.74428 ],\n",
       "         [ 82.968834, 114.10239 , 119.48631 ],\n",
       "         [101.39472 , 129.9126  , 135.0733  ],\n",
       "         ...,\n",
       "         [ 68.79668 , 100.6494  , 123.28986 ],\n",
       "         [ 83.27166 , 112.526115, 132.95065 ],\n",
       "         [ 77.137825, 102.578255, 124.11713 ]],\n",
       "\n",
       "        [[ 73.82483 , 100.      , 104.79465 ],\n",
       "         [ 81.35858 , 104.385284, 110.76921 ],\n",
       "         [ 87.98389 , 106.8233  , 113.8233  ],\n",
       "         ...,\n",
       "         [ 55.93082 ,  79.143486,  97.01102 ],\n",
       "         [ 80.08678 , 102.68938 , 118.45417 ],\n",
       "         [ 89.35728 , 105.560555, 125.47738 ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8925fe4e8191082b8190f4a8bc3e603dedcd139d4bece3a774bbecd6dab00e6b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
